# Machine Learning Terminology

* `Adam` = An optimizer that adjusts the learning rate based on the gradient information.
* `Backpropagation` = An algorithm used to train artificial neural networks in machine learning. It is a type of supervised learning that involves adjusting the weights and biases of a neural network based on the error between the predicted output and the true output.  The backpropagation algorithm works by first making a forward pass through the network to calculate the predicted output for a given input. The predicted output is then compared to the true output, and the error between the two is calculated using a loss function, such as mean squared error or cross-entropy.
* `Bias` = A learnable constant that is added to the weighted sum of the inputs of a neuron before the activation function is applied. The bias term allows the model to shift the output in a certain direction, independent of the input. This can be useful in cases where the input features do not have a strong signal or when the target variable is imbalanced.
* `Corpus` = Full dataset
* `Cross-Entropy` = A loss function to measure of the difference between two probability distributions and typically used with classification learning
* `Dataframe` = Object contain n key/value pairs where all values being equal length arrays.  When compard to a data table, the keys are the column names, the value of any key is that column's data, and the lentgh of the value array matches the number of rows in the table. 
* `DataLoader` = Utility for loading and batching data usually used with built in datasets such as MNIST or CIFAR, but also supports custom datasets 
* `Deep learning` = Training artificial neural networks with many layers, allowing them to learn complex patterns and representations of data.
* `Embedding` = Mapping from text tokens into a numerical form
* `Epoch` = One pass through the algorithm for the full dataset (not a single batch)
* `Gradient` = The derivative of a cost or loss function with respect to the model parameters. The cost or loss function measures the difference between the predicted output of a machine learning model and the actual output, and the gradient of this function provides the direction and magnitude of the steepest ascent or descent.
* `Gradient Descent` = Iterating towards the final weights and biases of the model
* `Iteration` = The number of batches required to complete one epoch
* `Learning Rate` = A hyperparameter that controls the step size at which a model's parameters are updated during training. The learning rate determines how much the weights of the model are adjusted in response to the error during training. If the learning rate is too high, the weights will be updated too aggressively, and the model may overshoot the optimal solution and diverge. On the other hand, if the learning rate is too low, the model may take too long to converge to the optimal solution, or it may get stuck in a local minimum and fail to find the global minimum.
* `Mean Square Error` = A loss function that penalizes larger errors more heavily, as it squares the differences between the predicted and true values. This means that the MSE is sensitive to outliers in the data, as they can contribute disproportionately to the overall loss. The MSE is commonly used for predicting house prices, stock prices, or medical diagnosis, where the goal is to minimize the average squared difference between the predicted and true values.
* `Neural Networks` = Layers of interconnected nodes or neurons that process and transform input data to produce an output. Each node is typically connected to several other nodes in the previous and/or next layer, and the connections between nodes are represented by weights and biases that are learned during the training process.
* `One Hot Encoding` = A technique used in machine learning to represent categorical data in a numerical format that can be easily used as input to machine learning models. In this technique, each category in a categorical variable is represented by a binary vector of 0s and 1s, with a 1 indicating the presence of a category and a 0 indicating the absence of a category.
* `Stochastic Gradient Descent (SGD)` = An optimizer to approximate the true gradient by using a smaller sample of the data at each iteration, which can be more computationally efficient than computing the gradient using the entire dataset. This can also lead to faster convergence and better generalization performance, as the model is updated more frequently and with different mini-batches.
* `Tensor` = A mathematical object that is used to represent multi-dimensional arrays of data.
* `Weight` = A parameter that is associated with a connection between two neurons in a neural network. The weight represents the strength of the connection between the neurons and is used to multiply the input signal from one neuron to another.
