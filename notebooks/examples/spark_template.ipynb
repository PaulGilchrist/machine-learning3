{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Configuration\n",
    "CLIENT_ID = ''\n",
    "CLIENT_SECRET = ''\n",
    "TENANT_ID = ''\n",
    "STORAGE_ACCOUNT_NAME = ''\n",
    "# create a Spark session with the Azure Data Lake Storage Gen1 or Gen2 configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AzureDataLakeQuery\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.11:0.6.0\") \\\n",
    "    .config(\"spark.hadoop.fs.adl.oauth2.access.token.provider.type\", \"ClientCredential\") \\\n",
    "    .config(\"spark.hadoop.fs.adl.oauth2.client.id\", CLIENT_ID) \\\n",
    "    .config(\"spark.hadoop.fs.adl.oauth2.credential\", CLIENT_SECRET) \\\n",
    "    .config(\"spark.hadoop.fs.adl.oauth2.refresh.url\", f\"https://login.microsoftonline.com/{TENANT_ID}/oauth2/token\") \\\n",
    "    .config(\"spark.hadoop.fs.adl.impl\", \"org.apache.hadoop.fs.adl.AdlFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "# Read data from Azure Data Lake Store\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .csv(f\"adl://{STORAGE_ACCOUNT_NAME}.azuredatalakestore.net/path/*.csv\")\n",
    "# Query the data using PySpark SQL\n",
    "df.createOrReplaceTempView(\"my_data\")\n",
    "result_df = spark.sql(\"SELECT * FROM my_data WHERE column1 = 'value1'\")\n",
    "# Show the query result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Execute an update query on the data\n",
    "result_df = spark.sql(\"UPDATE my_data SET column1 = 'new_value' WHERE column2 = 'value2'\")\n",
    "# Write the updated data back to the Azure Data Lake Store as a CSV file\n",
    "# .mode(\"overwrite\") is usually not recommended as it can be difficult to ensure data consistency and recoverability\n",
    "result_df.write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"adl://<your-data-lake-store-name>.azuredatalakestore.net/path/to/output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
